{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Least Squares\n",
    "\n",
    "We're now going to approach estimation with a non-linear state to measurement space mapping.\n",
    "\n",
    "$\n",
    "y  = h(x) + v\n",
    "$\n",
    "\n",
    "where $h(x)$ is a non-linear function and $v$ is a noise vector. \n",
    "\n",
    "As presented in class we cannot apply recursive estimation to the problem in it's current non-linear form. However, we can *linearize* the problem, allowing application of recursive estimation:\n",
    "\n",
    "$\n",
    "h(x) \\approx h(\\hat{x}_t) + H_{\\hat{x}_t}(x - \\hat{x}_t)\n",
    "$\n",
    "\n",
    "where $H_{\\hat{x}_t}$ is the Jacobian of h evaluated at $\\hat{x}_t$:\n",
    "\n",
    "This presents $h(x)$ as a linear function in the form of $Ax + b$ since $h(\\hat{x}_t)$ and $H_{\\hat{x}_t}$ are constant in this context. From here we can use recursive estimation the same as before. Note the *linearization* is only useful if $x$ is near $\\hat{x}_t$, otherwise the approximation quickly breaks down. This is why it's important to update the Jacobian frequently. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as LA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define $h(x)$ as:\n",
    "\n",
    "\n",
    "$h(x) = (f_{range}(x), f_{bearing}(x))$\n",
    "\n",
    "where \n",
    "\n",
    "$\n",
    "f_{range}(x) = sqrt({x_1}^2 + {x_2}^2) \\\\\n",
    "f_{bearing}(x) = atan2(x_2, x_1)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete implementation\n",
    "def f_range(x):\n",
    "    \"\"\"\n",
    "    Distance of x from the origin.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(x)\n",
    "\n",
    "# TODO: complete implementation\n",
    "def f_bearing(x):\n",
    "    \"\"\"\n",
    "    atan2(x_2, x_1)\n",
    "    \"\"\"\n",
    "    return np.arctan2(x[1],x[0])\n",
    "\n",
    "def h(x):\n",
    "    return np.array([f_range(x), f_bearing(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearize $h(x)$\n",
    "\n",
    "In order to linearize $h(x)$ you'll need the Jacobian:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{f_{range}}}{\\partial{x_1}} & \\frac{\\partial{f_{range}}}{\\partial{x_2}} \\\\\n",
    "\\frac{\\partial{f_{bearing}}}{\\partial{x_1}} & \\frac{\\partial{f_{bearing}}}{\\partial{x_2}} \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Remember to swap the derivative results of atan2 to match the swapped inputs ($atan2(x, y)$ vs $atan2(y, x)$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete jacobian of h(x)\n",
    "def jacobian_of_h1(x):\n",
    "    return np.array([        \n",
    "        [2*x[0], 2*x[1]],\n",
    "        [1.0 / (1.0 + (x[0]/x[1])**2), 1.0 / (1.0 + (x[1]/x[0])**2)]\n",
    "    ]).squeeze()\n",
    "\n",
    "def jacobian_of_h(x):\n",
    "    t = (1/2) * (x[0]**2 + x[1]**2) ** (-1/2)\n",
    "    return np.array([        \n",
    "        [t*2*x[0], t*2*x[1]],\n",
    "        \n",
    "        # atan2(x, y)\n",
    "        # ( y / (x^2 + y^2), ( -x / (x^2 + y^2)\n",
    "        # atan2(x, y)\n",
    "        # ( -x / (x^2 + y^2), ( $y / (x^2 + y^2)\n",
    "        [-x[0] / (x[0]**2 + x[1]**2), x[1] / (x[0]**2 + x[1]**2)]\n",
    "    ]).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! With the Jacobian of $h$ in your toolbox, you can plug it into recursive estimation.\n",
    "\n",
    "The update functions should look familiar ($H_{\\hat{x}_t}$ is the Jacobian of $\\hat{x}_t$).\n",
    "\n",
    "$\n",
    "Q_{t+1} = (Q_{t}^{-1} + H_{\\hat{x}_t}^T R^{-1} H_{\\hat{x}_t})^{-1} \\\\\n",
    "\\hat{x_{t+1}} = \\hat{x_t} + Q_{t+1} H_{\\hat{x}_t}^{T} R^{-1} (\\tilde{y_t} -  h(\\hat{x_t}))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "# Covariance matrix\n",
    "# added noise for range and bearing functions\n",
    "#\n",
    "# NOTE: these are set to low variance values\n",
    "# to start with, if you increase them you\n",
    "# might more samples to get\n",
    "# a good estimate.\n",
    "R = np.eye(2)\n",
    "R[0, 0] = 0.01\n",
    "R[1, 1] = np.radians(1) \n",
    "\n",
    "# ground truth state\n",
    "x = np.array([1.5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize $\\hat{x}_0$ and $Q_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat0 = np.array([3., 3]).reshape(-1, 1)\n",
    "Q0 = np.eye(len(x_hat0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Recursive Estimation\n",
    "def recursive_estimation(x_hat0, Q0, n_samples):\n",
    "    x_hat = np.copy(x_hat0)\n",
    "    Q = np.copy(Q0)\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "\n",
    "        # TODO: sample a measurement\n",
    "        # y_obs = h(x) + np.random.multivariate_normal([0, 0], R)\n",
    "        y_obs = h(x) + np.random.normal(0,1)\n",
    "\n",
    "        # TODO: compute the jacobian of h(x_hat)\n",
    "        H = jacobian_of_h(x_hat)\n",
    "\n",
    "        # TODO: update Q and x_hat\n",
    "        Q_inv = np.linalg.inv(Q)\n",
    "        R_inv = np.linalg.inv(R)        \n",
    "        Q = np.linalg.inv(Q_inv +  H.T @ R_inv @ H)\n",
    "        x_hat = x_hat + Q @ H.T @ R_inv @ (y_obs - H @ x_hat)        \n",
    "        # Q = \n",
    "        # x_hat = ?\n",
    "        \n",
    "    return x_hat, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x̂0 = [3. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(\"x̂0 =\", x_hat0.squeeze())\n",
    "\n",
    "x_hat, Q = recursive_estimation(x_hat0, Q0, n_samples)\n",
    "    \n",
    "print(\"x =\", x.squeeze())\n",
    "print(\"x̂ =\", x_hat.squeeze())\n",
    "print(\"Hx =\", h(x))\n",
    "print(\"Hx̂ =\", h(x_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "Ns = np.arange(0, 201, 5)\n",
    "for n in Ns:\n",
    "    x_hat, Q = recursive_estimation(x_hat0, Q0, n)\n",
    "    errors.append(LA.norm(x.squeeze() - x_hat.squeeze()))\n",
    "\n",
    "plt.plot(Ns, errors)\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Solution](./Non-Linear-Least-Squares-Solution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
